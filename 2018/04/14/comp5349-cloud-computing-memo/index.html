<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!--Description-->
  
  <meta name="description" content="Memos and notes of Ealon Huang">
  

  <!--Author-->
  
  <meta name="author" content="Ealon Huang">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="Comp5349 Cloud Computing Memo"/>
  
  <!--Open Graph Description-->
  
      <meta property="og:description" content="Memos and notes of Ealon Huang" />
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="Ealon&#39;s Blog"/>
  <!--Type page-->
  
      <meta property="og:type" content="article" />
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Title -->
  
  <title>Comp5349 Cloud Computing Memo - Ealon&#39;s Blog</title>


  <link rel="shortcut icon" href="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/ealon/ealon.png">

  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css">

  <!----------------------------
  https://github.com/GallenHu/hexo-theme-Daily

 _____            _   _
|  __ \          (_) | |
| |  | |   __ _   _  | |  _   _
| |  | |  / _` | | | | | | | | |
| |__| | | (_| | | | | | | |_| |
|_____/   \__,_| |_| |_|  \__, |
                          __/ |
                         |___/

    --------------------------->

</head>


<body>

  <!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/ealon/ealon.png" alt="Ealon's Blog" height="60">
        
      </a>
    </div>
    <!-- Navigation -->
    <nav class="navbar">
      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  HOME
                
              </a>
            </li>
          
            <li>
              <a href="/myapp">
                
                  MY APP
                
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  ARCHIVE
                
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            Comp5349 Cloud Computing Memo
            
          </h1>
          <p class="posted-on">
          2018-04-14
          </p>
          <div class="tags-links">
            
              
                <a href="/tags/usyd/" rel="tag">
                  usyd
                </a>
              
                <a href="/tags/linux/" rel="tag">
                  linux
                </a>
              
                <a href="/tags/hadoop/" rel="tag">
                  hadoop
                </a>
              
                <a href="/tags/python/" rel="tag">
                  python
                </a>
              
            
          </div>
        </div>
        <!-- Post Main Content -->
        <div class="entry-content">
          <hr>
<h1 id="Week4-MapReduce-Tutorial"><a href="#Week4-MapReduce-Tutorial" class="headerlink" title="Week4: MapReduce Tutorial"></a><strong>Week4: MapReduce Tutorial</strong></h1><p>In this lab, we will practice basic MapReduce Programming on locally installed <strong>Hadoop MapReduce</strong>. The lab starts by setting up the environment in the <strong>RedHat Linux workstation</strong> in uni’s lab as well as in the <strong>Ubuntu linux</strong> on my own Thinkpad.</p>
<p>Hadoop has grown into a large ecosystem since its launch in 2006. The core of Hadoop includes a distributed file system HDFS and a parallel programming engine MapReduce. Since Hadoop 2, YARN is included as the default resource scheduler.</p>
<h3 id="Step1-Prepare-the-environment"><a href="#Step1-Prepare-the-environment" class="headerlink" title="Step1: Prepare the environment"></a>Step1: Prepare the environment</h3><ol>
<li><p>Required software for Linux include:</p>
<ul>
<li>Java™ must be installed. Recommended Java versions are described at <a href="https://wiki.apache.org/hadoop/HadoopJavaVersions" target="_blank" rel="noopener">HadoopJavaVersions</a>. For my own laptop, I install it in <code>/usr/java/jre1.8.0_161</code></li>
<li>ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons. Install <strong>ssh</strong> and <strong>rsync</strong> by running<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install ssh</span><br><span class="line">sudo apt-get install rsync</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Install <a href="http://hadoop.apache.org/docs/r2.9.0/hadoop-project-dist/hadoop-common/SingleCluster.html" target="_blank" rel="noopener">Apache Hadoop 2.9.0</a></p>
<ul>
<li>Hadoop 2.9.0 is is already installed in <strong>RedHat Linux workstation</strong>  in uni’s labs.</li>
<li>For my own Thinkpad, download Hadoop 2.9.0, unpack the downloaded Hadoop distribution, install it by moving it to <code>/usr/local/hadoop-2.9.0</code>. </li>
</ul>
</li>
<li><p>In the distribution, edit the file <code>etc/hadoop/hadoop-env.sh</code> to define the parameters as follow:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># set to the root of your Java installation</span><br><span class="line">export JAVA_HOME=/usr/java/jre1.8.0_161</span><br></pre></td></tr></table></figure>
<p>then run </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/hadoop-2.9.0/bin/hadoop</span><br></pre></td></tr></table></figure>
<p>we should expect to see something similar as below:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> Usage: hadoop [--config confdir] [COMMAND | CLASSNAME]</span><br><span class="line"> CLASSNAME            run the class named CLASSNAME</span><br><span class="line">or</span><br><span class="line"> where COMMAND is one of:</span><br><span class="line"> fs                   run a generic filesystem user client</span><br><span class="line"> version              print the version</span><br><span class="line"> jar &lt;jar&gt;            run a jar file</span><br><span class="line">                      note: please use &quot;yarn jar&quot; to launch</span><br><span class="line">                            YARN applications, not this command.</span><br><span class="line"> checknative [-a|-h]  check native hadoop and compression libraries availability</span><br><span class="line"> distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span><br><span class="line"> archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span><br><span class="line"> classpath            prints the class path needed to get the</span><br><span class="line">                      Hadoop jar and the required libraries</span><br><span class="line"> credential           interact with credential providers</span><br><span class="line"> daemonlog            get/set the log level for each daemon</span><br><span class="line"> trace                view and modify Hadoop tracing settings</span><br><span class="line"></span><br><span class="line"> Most commands print help when invoked w/o parameters.</span><br></pre></td></tr></table></figure>
</li>
<li><p>Set our path variables. Create a directory call <em>comp5349</em> in your home directory <code>~/comp5349</code> and clone uni’s repository <code>lab_commons</code> locally. You will find it contains three sub directories: <code>profile</code>, <code>hadoop-conf</code> and <code>data</code>.</p>
<p>The login script template <code>LOGIN.TEMPLATE</code> is stored under <code>profile</code>. Copy this file under <code>/comp5349</code> and rename it as <code>.profile</code>. The template sets up all necessary environment variables, including future ones for Spark. It also updates the $PATH variable accordingly.<br>For uni’s <strong>RedHat Linux workstation</strong>,the content of <code>.profile</code> is:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> case $HOSTNAME in</span><br><span class="line"></span><br><span class="line">  soit-hdp-pro-*)</span><br><span class="line"></span><br><span class="line">      export JAVA_HOME=/usr/local/jdk1.8.0_40</span><br><span class="line">      export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">      export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</span><br><span class="line">      export SPARK_HOME=/usr/local/spark</span><br><span class="line">      ;;</span><br><span class="line">  w*)</span><br><span class="line">      export JAVA_HOME=/etc/alternatives/java_sdk</span><br><span class="line">      export HADOOP_HOME=/usr/local/hadoop-2.9.0</span><br><span class="line">      export HADOOP_CONF_DIR=~/comp5349/hadoop-conf</span><br><span class="line">      export HADOOP_LOG_DIR=~/comp5349/hadoop-logs</span><br><span class="line">      export YARN_LOG_DIR=~/comp5349/hadoop-logs</span><br><span class="line">export HADOOP_JHS_LOGGER=~/comp5349/hadoop-logs</span><br><span class="line">      export HADOOP_MAPRED_LOG_DIR=~/comp5349/hadoop-logs</span><br><span class="line">      export SPARK_HOME=/usr/local/spark-2.2.1-bin-hadoop2.7</span><br><span class="line">      export SPARK_CONF_DIR=~/comp5349/spark-conf</span><br><span class="line">      export SPARK_LOG_DIR=~/comp5349/spark-logs</span><br><span class="line">      ;;</span><br><span class="line"> esac</span><br><span class="line"> export PATH=.:$&#123;JAVA_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;SPARK_HOME&#125;/bin:/usr/local/anaconda3/bin:$&#123;PATH&#125;</span><br></pre></td></tr></table></figure>
<p>for my own laptop, change the content to </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jre1.8.0_161</span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop-2.9.0</span><br><span class="line">export HADOOP_CONF_DIR=~/comp5349/hadoop-conf</span><br><span class="line">export HADOOP_LOG_DIR=~/comp5349/hadoop-logs</span><br><span class="line">export YARN_LOG_DIR=~/comp5349/hadoop-logs</span><br><span class="line">export HADOOP_JHS_LOGGER=~/comp5349/hadoop-logs</span><br><span class="line">export HADOOP_MAPRED_LOG_DIR=~/comp5349/hadoop-logs</span><br><span class="line">export SPARK_HOME=/usr/local/spark-2.2.1-bin-hadoop2.7</span><br><span class="line">export SPARK_CONF_DIR=~/comp5349/spark-conf</span><br><span class="line">export SPARK_LOG_DIR=~/comp5349/spark-logs</span><br><span class="line">export PATH=.:$&#123;JAVA_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;SPARK_HOME&#125;/bin:/usr/local/anaconda3/bin:$&#123;PATH&#125;</span><br></pre></td></tr></table></figure>
<p><strong><em>The login script will be executed each time a new terminal window is opened.</em></strong> If you want the login script to take immediate effect in the current terminal window, run source .profile</p>
<p><img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/Screenshot%20from%202018-04-14%2016-03-22.png" alt=""></p>
</li>
<li><p>The script refers to several directories not yet exist. In particular, it specifies:</p>
<ul>
<li>Hadoop should look for configuration files in a directory <code>comp5349/hadoop-conf</code> under your home directory.</li>
<li>Both Hadoop and YARN and history servers should write log files in a directory <code>comp5349/hadoop-logs</code> under your home directory.<br>You can change the login script to use other directories for configuration and logging.</li>
</ul>
<p>In the following we assume you use the given login script. Do the following to prepare required directories:</p>
<ul>
<li>copy <code>hadoop-conf</code> directory from <code>lab_commons</code> to <code>comp5349</code></li>
<li>create a directory <code>hadoop-logs</code> under <code>comp5349</code></li>
</ul>
</li>
</ol>
<h3 id="Step2-Running-Sample-Example-in-Standalone-Mode"><a href="#Step2-Running-Sample-Example-in-Standalone-Mode" class="headerlink" title="Step2: Running Sample Example in Standalone Mode"></a>Step2: Running Sample Example in Standalone Mode</h3><ol>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source ~/comp5349/.profile</span><br><span class="line">unset HADOOP_CONF_DIR</span><br></pre></td></tr></table></figure>
</li>
<li><p>run </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.0.jar \</span><br><span class="line">wordcount \</span><br><span class="line">~/comp5349/lab_commons/data/place.txt \</span><br><span class="line">~/comp5349/countout</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Step3-Running-Sample-Example-in-Pseudo-Distributed-Mode"><a href="#Step3-Running-Sample-Example-in-Pseudo-Distributed-Mode" class="headerlink" title="Step3: Running Sample Example in Pseudo-Distributed Mode"></a>Step3: Running Sample Example in Pseudo-Distributed Mode</h3><ol>
<li><p>Edit the file <code>$HADOOP_CONF_DIR/hadoop-env.sh</code> to define the parameters as follow, as we did in Step1.3:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># set to the root of your Java installation</span><br><span class="line">export JAVA_HOME=/usr/java/jre1.8.0_161</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/comp5349/.profile</span><br></pre></td></tr></table></figure>
</li>
<li><p>Check that you can ssh to the localhost without a password:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh localhost</span><br></pre></td></tr></table></figure>
<p>If you cannot ssh to localhost without a password, execute the following commands to generate a key to identify yourself:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa</span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line">chmod 0600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>
<p> Check again to ensure you can ssh to localhost without supplying password.</p>
<blockquote>
<p>make sure <strong>ssh</strong> is installed and enabled.</p>
</blockquote>
</li>
<li><p>Start HDFS</p>
<p> We assume that your HDFS should store data in <code>/comp5349/hadoop-data</code>. Run the following commands to prepare the directories:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p ~/comp5349/hadoop-data/dn</span><br><span class="line">mkdir -p ~/comp5349/hadoop-data/nn</span><br></pre></td></tr></table></figure>
</li>
<li><p>Configure <code>$HADOOP_CONF_DIR/hdfs-site.xml</code></p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///home/yilong/comp5349/hadoop-data/dn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file:///home/yilong/comp5349/hadoop-data/nn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Format the file system with:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>
<p> Then start all HDFS services with</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/sbin/start-dfs.sh</span><br></pre></td></tr></table></figure>
<p> Check that HDFS is successfully started by visiting the HDFS Web UI <a href="http://localhost:50070/" target="_blank" rel="noopener">http://localhost:50070/</a> in your browser, you should be able to see something similar to the pictures below<br> <img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/50070_1.png" alt=""><br> <img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/50070_2.png" alt=""></p>
</li>
<li><p>A few directories need to be prepared for executing Hadoop jobs:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /user</span><br><span class="line">hdfs dfs -mkdir /user/yilong</span><br></pre></td></tr></table></figure>
</li>
<li><p>Start YARN. YARN is the preferred resource scheduler for MapReduce. We have prepared the configuration files to run YARN on the pseudo cluster. To start YARN:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>
<p> To check that YARN has been started successfully, visit the YARN WebUI at <a href="http://localhost:8088/" target="_blank" rel="noopener">http://localhost:8088/</a> in your browser. You will see an output similar to<br> <img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/Screenshot%20from%202018-04-14%2017-20-21.png" alt=""></p>
</li>
<li><p>Create a directory on HDFS to save application logs</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir -p /var/log/hadoop-yarn/apps</span><br></pre></td></tr></table></figure>
<p> Start a job history server so we have a web UI to access job logs:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>
</li>
<li><p>Run sample program.<br>The default file system for Hadoop pseudo cluster operation is HDFS. In this exercise, HDFS will be used for program’s input and output.<br>Run the following command to put the <code>place.txt</code> file in HDFS, under your home directory.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put ~/comp5349/lab_commons/data/place.txt place.txt</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/Screenshot%20from%202018-04-14%2017-26-49.png" alt=""></p>
</li>
<li><p>The following command runs the grep example on place.txt to look for all records with the word “Australia” in it</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">$HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.0.jar \</span><br><span class="line">grep \</span><br><span class="line">place.txt \</span><br><span class="line">placeout \</span><br><span class="line">&quot;/Australia[\d\w\s\+/]+&quot;</span><br></pre></td></tr></table></figure>
<p><img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/Screenshot%20from%202018-04-14%2017-29-37.png" alt=""><br><img src="https://raw.githubusercontent.com/Ealon/ealon.github.io/master/images/2018/Apr/Screenshot%20from%202018-04-14%2017-31-07.png" alt=""></p>
</li>
<li><p>To stop all services, run </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh stop historyserver</span><br><span class="line">$HADOOP_HOME/sbin/stop-yarn.sh</span><br><span class="line">$HADOOP_HOME/sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Step4-A-Customized-Sample-Program"><a href="#Step4-A-Customized-Sample-Program" class="headerlink" title="Step4: A Customized Sample Program"></a>Step4: A Customized Sample Program</h3><ol>
<li> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put ~/comp5349/lab_commons/data/partial.txt partial.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>Use Uni’s python_resources week4. This folder contains two implementations of counting tags used by users task:</p>
</li>
</ol>
<ul>
<li><p><strong>native</strong>: this folder contains an naive implementation, containing only the map and reduce phase. <code>tag_driver.sh</code> is a simple script provided as an example of using streaming API. It takes two arguments: the <strong><em>input file</em></strong> and the <strong><em>output path</em></strong>. Below is an example:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tag_driver.sh partial.txt out</span><br></pre></td></tr></table></figure>
<blockquote>
<p>before executing, make sure that <code>tag_driver.sh</code> is executable by running</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod a+x tag_driver.sh</span><br></pre></td></tr></table></figure>
</blockquote>
<p>  The output in terminal is</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">18/04/14 23:26:18 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.</span><br><span class="line">packageJobJar: [tag_mapper.py, tag_reducer.py, /tmp/hadoop-unjar6850475693049103585/] [] /tmp/streamjob1779370246496388406.jar tmpDir=null</span><br><span class="line">18/04/14 23:26:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">18/04/14 23:26:19 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">18/04/14 23:26:20 INFO mapred.FileInputFormat: Total input files to process : 1</span><br><span class="line">18/04/14 23:26:20 INFO mapreduce.JobSubmitter: number of splits:2</span><br><span class="line">18/04/14 23:26:20 INFO Configuration.deprecation: yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled</span><br><span class="line">18/04/14 23:26:21 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1523708485184_0001</span><br><span class="line">18/04/14 23:26:21 INFO impl.YarnClientImpl: Submitted application application_1523708485184_0001</span><br><span class="line">18/04/14 23:26:21 INFO mapreduce.Job: The url to track the job: http://yilong:8088/proxy/application_1523708485184_0001/</span><br><span class="line">18/04/14 23:26:21 INFO mapreduce.Job: Running job: job_1523708485184_0001</span><br><span class="line">18/04/14 23:26:29 INFO mapreduce.Job: Job job_1523708485184_0001 running in uber mode : false</span><br><span class="line">18/04/14 23:26:29 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">18/04/14 23:26:36 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">18/04/14 23:26:43 INFO mapreduce.Job:  map 100% reduce 33%</span><br><span class="line">18/04/14 23:26:45 INFO mapreduce.Job:  map 100% reduce 67%</span><br><span class="line">18/04/14 23:26:46 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">18/04/14 23:26:46 INFO mapreduce.Job: Job job_1523708485184_0001 completed successfully</span><br><span class="line">18/04/14 23:26:46 INFO mapreduce.Job: Counters: 50</span><br><span class="line">    File System Counters</span><br><span class="line">        FILE: Number of bytes read=7481</span><br><span class="line">        FILE: Number of bytes written=1042039</span><br><span class="line">        FILE: Number of read operations=0</span><br><span class="line">        FILE: Number of large read operations=0</span><br><span class="line">        FILE: Number of write operations=0</span><br><span class="line">        HDFS: Number of bytes read=10910</span><br><span class="line">        HDFS: Number of bytes written=5603</span><br><span class="line">        HDFS: Number of read operations=15</span><br><span class="line">        HDFS: Number of large read operations=0</span><br><span class="line">        HDFS: Number of write operations=6</span><br><span class="line">    Job Counters </span><br><span class="line">        Killed reduce tasks=1</span><br><span class="line">        Launched map tasks=2</span><br><span class="line">        Launched reduce tasks=3</span><br><span class="line">        Data-local map tasks=2</span><br><span class="line">        Total time spent by all maps in occupied slots (ms)=8670</span><br><span class="line">        Total time spent by all reduces in occupied slots (ms)=16392</span><br><span class="line">        Total time spent by all map tasks (ms)=8670</span><br><span class="line">        Total time spent by all reduce tasks (ms)=16392</span><br><span class="line">        Total vcore-milliseconds taken by all map tasks=8670</span><br><span class="line">        Total vcore-milliseconds taken by all reduce tasks=16392</span><br><span class="line">        Total megabyte-milliseconds taken by all map tasks=8878080</span><br><span class="line">        Total megabyte-milliseconds taken by all reduce tasks=16785408</span><br><span class="line">    Map-Reduce Framework</span><br><span class="line">        Map input records=73</span><br><span class="line">        Map output records=331</span><br><span class="line">        Map output bytes=6801</span><br><span class="line">        Map output materialized bytes=7499</span><br><span class="line">        Input split bytes=194</span><br><span class="line">        Combine input records=0</span><br><span class="line">        Combine output records=0</span><br><span class="line">        Reduce input groups=233</span><br><span class="line">        Reduce shuffle bytes=7499</span><br><span class="line">        Reduce input records=331</span><br><span class="line">        Reduce output records=233</span><br><span class="line">        Spilled Records=662</span><br><span class="line">        Shuffled Maps =6</span><br><span class="line">        Failed Shuffles=0</span><br><span class="line">        Merged Map outputs=6</span><br><span class="line">        GC time elapsed (ms)=698</span><br><span class="line">        CPU time spent (ms)=5690</span><br><span class="line">        Physical memory (bytes) snapshot=1175052288</span><br><span class="line">        Virtual memory (bytes) snapshot=9947533312</span><br><span class="line">        Total committed heap usage (bytes)=761790464</span><br><span class="line">    Shuffle Errors</span><br><span class="line">        BAD_ID=0</span><br><span class="line">        CONNECTION=0</span><br><span class="line">        IO_ERROR=0</span><br><span class="line">        WRONG_LENGTH=0</span><br><span class="line">        WRONG_MAP=0</span><br><span class="line">        WRONG_REDUCE=0</span><br><span class="line">    File Input Format Counters </span><br><span class="line">        Bytes Read=10716</span><br><span class="line">    File Output Format Counters </span><br><span class="line">        Bytes Written=5603</span><br><span class="line">18/04/14 23:26:46 INFO streaming.StreamJob: Output directory: out</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>combiner</strong>: this folder contains the version using combiner. You will notice that the mapper implementation (<em>tag_mapper.py</em>) is quite similar, the only difference is the output format. In the naive version, we have </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"&#123;&#125;\t&#123;&#125;"</span>.format(tag, username))</span><br></pre></td></tr></table></figure>
<p>  and in the combiner version, we have </p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"&#123;&#125;\t&#123;&#125;=1"</span>.format(tag, username))</span><br></pre></td></tr></table></figure>
<p>  The reducer implementations also reflect the changed input value part.  We also provide a script <em>tag_driver_combiner.sh</em> showing you how to indicate combiner with option <em>-combiner</em> followed by the actual combiner script, which is also the reducer script <em>tag_reducer.py</em>. The usage of <em>tag_driver_combiner.sh</em> is exactly the same as <em>tag_driver.sh</em>. Below is an example:</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tag_combiner_driver.sh partial.txt out_combiner</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>To find out more about various options for use in hadoop streaming, see <a href="http://hadoop.apache.org/docs/current/hadoop-streaming/HadoopStreaming.html" target="_blank" rel="noopener">Hadoop Streaming</a></p>

        </div>
      </article>
    </div>
    <!-- Comments -->
    <div class="container">
      
<section id="comment">
  <!-- <h1 class="title">Comments</h1> -->

  
</section>


    </div>
    <!-- Pre or Next -->
    <div class="nav-links">
      
        <div class="nav-previous">
          <a href="/2018/04/17/pte-note-speaking-and-writing/" rel="prev"><span class="meta-arraw meta-arraw-left"></span> Previous Post</a>
        </div>
      
      
        <div class="nav-next">
          <a href="/2018/04/14/ubuntu-quick-config-memo/" rel="prev">Next Post <span class="meta-arraw meta-arraw-right"></span></a>
        </div>
      
    </div>

  </div>
</div>


  <!-- Footer -->
  <!-- Footer-widgets -->
<div class="footer-widgets">
  <div class="row inside-wrapper">
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">ABOUT</h1>
        <div class="custom-widget-content">
          
          Memos and notes of Ealon Huang
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Contact</h1>
        <div class="widget-text">
          
            
              <a href="https://github.com/Ealon" class="icon icon-github" target="_blank">github</a>
            
              <a href="" class="icon icon-wechat" target="_blank">wechat</a>
            
              <a href="mailto:ealonhuang@gmail.com" class="icon icon-gmail" target="_blank">gmail</a>
            
              <a href="" class="icon icon-facebook" target="_blank">facebook</a>
            
          
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">Search</h1>
        <div class="widget-text">
          <form onSubmit="return appDaily.submitSearch('')">
            <p>
              <input type="text" placeholder="search..." id="homeSearchInput">
            </p>
            <!-- <input type="submit" value="GO"> -->
          </form>
        </div>
      </aside>
    </div>
  </div>
</div>
<!-- Footer -->
<footer class="site-info">
  <p>
    <span>Ealon's Blog &copy; 2018</span>
    
      <span class="split">|</span>
      <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> with Theme <a href="https://github.com/GallenHu/hexo-theme-Daily" target="_blank">Daily</a></span>
    
  </p>
</footer>


  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>





</body>

</html>